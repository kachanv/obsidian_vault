технологии, фреймворки, СУБД для обсчета и преобразования данных

### MapReduce
**Опр:** модель распределенных параллельных вычислений в распределенной файловой системе
* реализуется в распределенной файловой системе (ФС)
	* обычно для такой ФС не нужно специальное железо, узлы могут общаться через интернет
	* то, как именно ФС будет распределять данные по узлам - в общем виде не важно (HDFS распределяет случайно-равномерно) 
* все расчеты, конечные и промежуточные пишутся на диск: можно перезапустить расчет в случае падения, очень много операция I/O из памяти диска
* "где данные там и вычисления"
* операции
	* *Map* - локально на каждой ноде происходит параллельное выполнение функции высшего порядка map() над локальными данными ноды
		* map возвращает на выходе *отсортированные* пары key-value
	* *Shuffle* - перемещение данных между узлами, чтобы дальнейший шаг выполнился локально
	* *Sort* - до сортировка данных на каждой ноде
	* *Conbine* - пред агрегация результатов на каждой ноде локально
	* *Reduce*- агрегация итогов на master-node
		* принимает на вход *отсортированные* пары key-value
* output каждой операции это input для следующей
* проблема с перекосами решается специальными алгоритмами когда "горячие" ключи обрабатываются отдельно
* входные данные не изменяемы
	* использовался в *Pig*, *Hive*
* задачи синхронны т.е. следующая запускается только после полного завершения предыдущей

### Spark
* фреймворк, альтернатива MapReduce, более быстрая
* промежуточные данные в RAM
* интерфейсы на всех популярных ЯП
* RDD - расчеты и запись производятся только "по надобности" aka yeld в python
* может запускать подзадачи асинхронно в отличие от *MapReduce*
* более универсален, покрывает массу задач пакетной и потоковой обработки 
### Tez
* фреймворк, альтернатива MapReduce от Hortonworks
* используя YARN объединяет MapReduce задачи в одно задание, рассматриваемое как DAG
* промежуточные данные в RAM
* задача представляется как DAG - направленный ацикличный граф, где вершины - обработчики данных; граф динамически пере конфигурируется в процессе 
* выполнения в зависимости от данных
* может запускать подзадачи асинхронно в отличие от *MapReduce*

### Impala
* MPP СУБД над распределенной файловой системой
* СУБД от Cloudera
* свой движок (не MapReduce или Tez)
* кэширование и колоночное хранение
* 

### Hive
* первая появившаяся и самая популярная СУБД для hdfs
* HiveQL - урезанный SQL
* последние версии работают на Tez
* как выглядит партиционирование в hive:
	* каждая партиция в отдельной директории hdfs
	* не влияет на распределение данных по узлам
	* каждое уникальное значение полей партиционирования ложится в отдельную партиция (без диапазонов)
### Presto

### Vintage
* форкнутая версия Trino под нужды Tinkoff
	* *как trino виртуализирует данные ?* 

### Cassandra
* NOSQL
* хранит данные в виде хранилищ ключей и значений (табличная структура не используется в реальном хранилище)
* каждая строка таблицы может содержать свой набор столбцов
* данные хранятся в виде SSTable на диске
#### Запись данных
* рассмотрим данные поступающие в 1 таблицу
* все данные сначала фиксируются в append-only commit log на диск. С целью восстановления в случае падения. log чистится после записи данных в SSTable
* данные с commit log отправляются в память в Memtable 
	*Memtable* - плоская таблица в RAM. Данные в которой уже отсортированы (обычно используется *RB tree*).  1 таблица - 1 memtable. 
* когда commit log достигает предельного значения данные из memtable сбрасываются в SSTable на диск.
	*SSTable* состоит из отсортированной, иммутабельной последовательности пар ключ-значение, где каждый ключ уникален и соответствует одному значению. SStable состоит из нескольких файлов: данные (внутри ещё разбиты по патрициям), индекс, мета, *фильтр Блума*. 
* Новая свежая SStable объединяется со старой, поскольку данные внутри отсортированы - объединение происходит быстро.