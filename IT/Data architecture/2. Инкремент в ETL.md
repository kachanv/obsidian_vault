Способ правильно и эффективно организовать инкремент зависит от всех компонентов ETL: вид источника, логика самого etl процесса, среды где будет все это выполняться, даже от формата данных таргета.
**Цель:** минимизировать расход ресурсов для достаточно корректного применения изменений СИ на ETL.

**Глоссарий**
**Change Data Feed (CDF)** - функция позволяющая получить лог изменений между версиями таблицы. т.е. задаётся таблица и 2 метки времени, на выходе весь набор SQL команд который нужно применить к 1 версии таблицы чтобы получить вторую.

**Допущения**: 
1. рассмотрим только случаи захвата изменений из реляционной БД
2. будем разделять **захват изменений**, **построение инкремента**, и **применение этого среза на таргет** как 3 раздельные задачи
3. пусть трансформация данных до нужного типа SCD - часть логики ETL процесса, мы не рассматриваем её подробно
4. операции применения инкремента на таргет всегда выполняем со сравнением т.е. не применяем изменение если строка в таргете ETL остается неизменной
5. физическое или логическое удаление строки транслируется в таргет ETL с помощью логического флага удаления т.е. помечаем удаленные строки а не удаляем их физически

**Требования**
1. *идемпотентность* - повторный запуск приведет к тому же результату при тех-же входных параметрах и данных
2. *минимальный срез данных* - в инкремент попадает минимальный набор данных необходимый и достаточный для отражения изменений на ETl таргет

**Проблемы**
1. *обработка физических удалений* - как поймать физическое удаление строки
2. *обработка запоздавших данных* - сильный разрыв между приходом данных в ETL процесс и их физическом появлении на источнике
3. *каскадная обработка инкремента* - каждый следующий ETL процесс должен корректно применять инкремент предыдущего ETL процесса (агрегации, дедупликации, сложная логика расчета)
4. *быстро меняющиеся данные* - установка последовательности применения изменений если они происходят в 1 единицу времени

**Параметры источника**
возможность выбора способа построения инкремента зависит от характеристик источника а именно:
* наличие **PK** или иного способа однозначно идентифицировать строку
* наличие счетчика изменения - можно идентифицировать что строка изменилась/появилась с последней загрузки
* тип операции - insert | update | delete
* возможность точечно идентифицировать изменение строки - изменение конкретного поля (битовая маска)
* наличие предыдущего, измененного значения в логе - если в логике расчета важно сравнивать текущее и предыдущее значение
* наличие **CDF**

**Способы захвата изменений строки**
* если нельзя идентифицировать строку, даже суррогатным ключом - то никак
* если есть счетчик изменений - можно сохранять его на стороне ETL и выгружать изменения
* если есть **CDF** - достаточно попросить изменения с последней выгрузки
* сравнивать строки по **hash** или **each2each**
* чекать триггер/fingerprint/max(update_dttm) таблицы на наличие изменений в ней

**Построение среза**
* если для корректного расчета в таргете ETL не нужна ретроспектива по ключу -захваченные изменения = срез
* если нужна ретроспектива - она достается из источника или ETL таргета. 
	* в случае составного ключа, возможен захват изменений по части ключа, а построение среза по полному ключу, с помощью джоина на таргет/источник
	* кейсы, когда может понадобится ретроспектива по ключу
		* историчность ETL таргета срезами
		* агрегация | сортировка
		* сравнение текущего/предыдущего значения для расчета

**Применение инкремента на таргет**
* НЕ отслеживаем изменение/удаление - **insert only** (со сравнением, если хотим идемпотентности)
* отслеживаем изменение, НЕ отслеживаем удаление - **update & insert** | **delete & insert**
* отслеживаем изменение, отслеживаем удаление, можем выделить удаленные строки - **update & insert** | **delete & insert**
* не отслеживаем изменение, отслеживаем удаление, можем выделить удаленные строки - **delete & insert**
* отслеживаем удаление, не можем выделить удаленные строки - **полная выгрузка** + джоин на таргет
* отслеживаем удаление, не можем выделить удаленные строки - **полная выгрузка** + джоин на таргет
* не можем идентифицировать порядок применения изменений в источнике - **полная выгрузка** + джоин на таргет (если отслеживаем удаления)
* не можем идентифицировать строку, отслеживаем изменение - **replace**

**Способы сравнения изменения атрибутов**
* трансляция с источника - если на источнике можно точечно идентифицировать изменения (битовая маска, before_update)
* сравнения hash функции по всем атрибутам - быстрее чем сравнивать все атрибуты между собой (проблема коллизий)

**Задача**
SOURCE:
1. Сотрудник (*emp_id*)
2. Должность (*pos_id*)
3. Зарплата (*sal*)
4. Флаг удаления (*del_flg*)
TARGET:
1. *pos_id*
2. avg(*sal*)
3. *del_flg*
В табличке SOURCE физических удалений не бывает, только логические через флаг удаления. Сотрудники могут менять должность, менять зарплату, увольняться.
Придумайте алгоритм (можно без кода, просто на словах), как бы вы перестраивали этот отчёт инкрементально: по возможности без обновления всех записей каждый день. отдельно рассмотреть случаи scd1 и scd2 источников

**Анализ и допущения**
* далее исходим из того что мы не можем использовать никакие другие поля для решения задачи, есть только-то что есть в условии
* нам недоступен лог изменения таблицы SOURCE или CDF  и прочие функции, только функционал SQL-92

Задачу можно разбить на 3 шага: **1**-захват изменений, **2**-построение среза, **3**-применение среза на таргет
**1 - захват изменений** 
*будет выигрышным если инструмент быстро join-ит*
* после init загрузки заводим таблицу Т1: *emp_id* (PK) | *pos_id* | hash (*pos_id* | *sal* | *del_flg*). т.о. это копия последнего выгруженного ТИ
* при выгрузке сравниваем по *emp_id* из T1 изменились ли атрибуты в SOURCE с помощью hash. Получаем 3 группы *emp_id*: Новые | Изменились | Не изменились. т.о. размечаем SOURCE
* также размечаем SOURCE полем *pos_id* из T1, называем *pos_id_old*

**2 - построение среза** 
* нам нужно учесть кейсы когда у сотрудника меняется должность т.о. нам нужно захватить и пересчитать *avg(sal)* по старой должности
* из размеченного SOURCE выгружаем все *pos_id* у которых группа: Новые | Изменились и те уникальные *pos_id_old* которые изменились но которых нет в пред. списке
* полученный список inner join на SOURCE по *pos_id*, получаем срез для расчета

**3 - применение среза на таргет**
* полученный срез прогоняем через логику джоба
* новые ключи вставляем, старые - обновляем только если изменились атрибуты
* транкейтим табличку T1, заново наполняем её из SOURCE
* del_flg считаем либо по сумме строк (sum=count=>1), либо по sum(sal), если =0 то 1

**В случае SCD2 источника**
* зависит от доступного набора SCD2: поля версионности | возрастающий сиквенс записи | флаг актуальности
* если есть неизменяемая версионность или сиквенс => сохраняем последний выгруженный по ключу и забираем его
* если версионность может меняться задним числом - контрольные суммы по ключу или хэширование отсортированного списка атрибутов по ключу