### schedule
1. 10:15-10:05 | *Зал 13. Найроби+Касабланка (2 этаж)* Динтаблицы YTsaurus — и ещё одна СУБД от Яндекса.
2. 11:20-12:10 | *Зал 13. Найроби+Касабланка (2 этаж)* Инкрементальные бэкапы в PostgreSQL при помощи Ptrack и Walsummarizer, или Bloom filter vs. roaring bitmap
3. 12:25-13:15 | *Зал 01. Конгресс-холл* Надежность на масштабе в 45 млн клиентов — инструменты и практики цифрового банка
4. 13:30-14:20 | *Зал 01. Конгресс-холл* Транзакционная работа с топиками. Архитектура и сравнение решений в Apache Kafka и YDB
5. 14:40-15:30 | *Зал 10. Рио-де-Жанейро* UDF к Broadcast Join и обратно. История одной Spark-оптимизации
6. 15:50-16:40 | *Зал 10. Рио-де-Жанейро* Оптимизация Spark-приложений от простого к сложному. С примерами
7. 17:00-17:50 | *Зал 11. Белу-Оризонти (2 этаж)* Data Quality против всех
8. 18:10-19:00 | *Зал 10. Рио-де-Жанейро* Как мы заменили сотни Join’ов на один РТ-процессинг с 1kk RPS 

1. 10:00-10:50 | *Зал 11. Белу-Оризонти (2 этаж)* Как на самом деле работает Apache Iceberg
2. 11:10-12:00 | *Зал 01. Конгресс-холл* Как объединять данные из разных СУБД и делать это эффективно
3. 12:20-13:10 | *Зал 05. Мумбаи* *(2 этаж)* Стоимостный оптимизатор в YDB — как, зачем и почему?
4. 13:30-14:20 | *Зал 03. Дели+Калькутта / Яндекс-трек* Эволюция аутентификации в SSH: от паролей до наших дней
5. 14:40-15:30 | *Зал 02. Пекин+Шанхай* Picodata: много маленьких данных
6. 15:50-16:40 | *Зал 15. Кластер Индия / Devhands.io* Room Распределённые системы без «зауми»
7. 17:00-17:50 | *Зал 02. Пекин+Шанхай* Итак, вы решили надежно записывать данные на диск
8. 18:10-19:00 | *Зал 12. Кейптаун / PHP* Что происходит на рынке труда?

## Ytsaurus
* началось все с **YAMR** — Yet Another MapReduce. 2006
* Open source с 2023
* SDK: C++, Java, Go и Python
* *Cypress* - Distributed File System + дерево метаинформации. Не фремворк тестирования.
* *Static Table* 
	* "статические" подразумевает, что данные, которые уже записаны в таблицу, невозможно обновить без перезаписи
	* ACID, но с условиями
	* блокировки (snapshot, exclusive и shared)
	* FK отсутсвуют
* *Dynamic Table* 
	* интерфейс точечного чтения и записи данных по ключу
	* разные форматы хранения для key-value, OLTP и OLAP сценариев
	* MVCC
	* уровень изоляции "snapshot", как в MySQL (взаимоблокировки и падение транзакций если они меняют одни данные)
	* LSM для вставки и хранения (?)
* *CHYT* - запускать кластера CH для работы с данными в YTsaurus
* *SPYT* - аналогично для Spark
* MapRaduce поверх Oltp
* OLTP распределённые транзакции, в том числе между разными таблицами
* Поколоночное хранение, всегда, даже в OLTP (?)
* OLTP + OLAP
* отказоустойчивость:  свой алгоритма консенсуса (RSM), схожая с RAFT
* какая то серебрянная пуля, все умеет, все плюшки OLTP с возможносьбю аналитики поверх
![[Pasted image 20241211214421.png|700x400]]

## Транзакционная работа с Kafka топиками
- задача: сервис переотправки данных из двух (1,2) топиков в два других (3,4). Заглавные и строчные буквы.
- сревис должен: прочитать из топика (A), записать в топик (B), закоммитить оффсет в топик (A). Если падает запись в (B) оффсет не коммититься.
![[Снимок экрана 2024-12-14 162630.png|900x300]]
### Проблемы:
- как откатывать что-либо из журнала ?
- т.к. топики содержат n партиций: задача ещё усложняется, нам нужно откатить изменения со всех партиций, если не удалось записать хоть в 1 из них
### Настройик Consume
- отключение автокоммита (коммит управляется сервисом)
- isolation_level = «read_commited» (в 2017 появилась семантика exactly-once)
### Настройик Produce
- нам нужна семантика (exactly-once: enable_idempotence = True)
- для решения этой проблемы с партициями (и не только), есть координатор (Replicated State Machine, модуль внутри каждого брокера Kafka) и журнал транзакций (топик __transaction_state, хранящий последнее состояние транзакции)
- на продюсере каждому сообщению назначается transactional_id, который следит, и если сообщение зпаисалось не на все партиции, помечает его как невалидное, потребитель не увидит (isolation_level = «read_commited»)
### Оффтоп:
- в библиотеки есть встроенная поддержка: aiokafka - Transactional Consume-Process-Produce
- полноценная защита от конфликта трнзакций появилась в Kafka 3.7 (2024): механизм поколения транзакций.
- в 1 топик возможны запись транзакционных и нетранзакционных сообщений (для транзакционных отдельно в топик публикуется сообщение конца транзакции и в мету сообщения пишется transaction_id)
## YDB стоимостной оптимизатор
Павел Велихов пишет оптимизаторы 15 лет, 6 СУБД, 4 MPP
### YDB
10 лет, надоело мигрировать
* СУБД: редяционная модель горизонтально масштабируемая, MPP, распределенная (кросс-датацентровая), отказоустойчивая.
* open source
* строгая консистентность (настраиваемая, на уровне базы, падение производительности)
* Isolation - serializable (настраиваемая). MVCC
* честный ACID гарантируется только в рамках таблиц одного типа
* изначально OLTP, потом внедрили OLAP. Больше для OLTP
* колоночное хранение, векторный движок (HTAP = OLAP + OLTP, план на будущее)
* сценарий - одна база, OLTP таблица реплицируется в OLAP (колоночную), оптимизатор при запросе выбирает нужную таблицу в зависимости от запроса
#### Оффтоп
* shared nothing
* данные шардированы по узлам
* раздельный compute и storage
* все данные сортируются по PK
* все данные партиционируются (row - по range PK, column - Partition key)
* *таблетка* - LSM-tree над куском данных таблицы, ветви ссылаются на blob в distributed storage
### Стоимостный оптимизатор
* OLTP может жить с плохим оптимизатором, OLAP нет
* план должен строится не более секунды
* энумератор - перебирает возможные способы преобразования данных - далее оценочной функцией выбирается самый дешевый план
	* перебирать всё всё - дорого (n!), те бд кто делают так, не могут построить план для 30+ джоинов (TOP DOWN оптимизатор)
	* хорошие оптимизаторы по умному перебирают дерево планов, позволяя оптимально джоинить 100+ таблиц, YDB использует (BOTTOM DOWN) оптимизатор
* конечно имеет значение не только количество джоинов но и их граф т.е. что с чем соединятеся
* оценка кардинальности - самая сложная часть оптимизатора, с каждой операции накапливается ошибка для следующего шага
* есть полуготовые продукты где оптимизатор - *ML*, проблема в риске регрессии в любой момент что недопустимо для enterprice
* оптимизатор используется для *workload manage*r, чтобы убивать плохие запросы до запуска
	* *workload manager* - квотирование и разграничение ресурсов, очереди
* традиционные бенчмарки для оценки оптимизаторов *TCPH* и ещё две, они все равно не подходят для адекватной оценки
* YDB использует только количество строк и кардинальность по PK
* плохо работает с лишними MOTION, *GP* лучше, он выкидывает лишние движения данных
* есть хинты но хотят убрать со временем (в *GP* есть хинты ? *GP* плохо работает explain analyze ?)
![[Снимок экрана 2024-12-14 182013.png|800x350]]
Лучшие оптимизаторы - Teradata, Vertica, потом Oracle и Redshift, самое дно - GP (orca - top down, старый) и Trino
## Инкрементальные бэкапы в PostgreSQL
* pg_probackup - старая всем известная утилита
* во время восстановления бд недоступна  - первая поетря
* дельта между последним бекапом и сегодня - вторая потеря
* wallsunarazer - новая утилита создания и наката бекапов в винильном PG
* roaring bitmap как структура  для хранения
* +- одинаковые бенчмарки
* перед восстановлением бекапы проходят валидацию

## Надежность на масштабе в 45 млн клиентов
* site relabiliyu engeneering книжка для SRE
* SLI
* SLO
* SLA
* Бюджет ошибок
* MTTR
* MTBF
* без мониторинаг нет конроля надежности
* нужно управлять релизами

## UDF к Broadcast Join и обратно. История одной Spark-оптимизации
* елси udf написана на PySpark есть оверхед на сериализацию
* такая udf выполняется на локальных py, однопоточно (?)
* нативный df api рабоатет на Scala

## Оптимизация Spark
* репартицирование
* динамическое изменение параметров (напр. размер блока репартицирования). Используя метаданные файлов.
* использовать set/list вместо broadcast join
* spillit оконку если по ключу больше 4096 строк def
* предрасчет аггрегатов + set будет эффективнее чем оконка
* нельзя в рпмках одного приложения сделать несколько сессий

## Data Quality
* все данные плохие. Нужно писать проверки на нормальность данные, а не на аномалии
* dbt test, Soda Core - готовые решения из коробки

## Заменили сотни Join’ов на один РТ-процессинг с 1kk RPS
* яндекс решает сложные интересные задачки

## Apache Iceberg
* киллер фияа по сравнению с hive - мета в одном месте, консистентное чтение
* транзакционность в DL 
	* атомарность на уровне таблицы, не строки. Транзакционность батчевая
	* кросстабличные транзакции не поддерживаются, только в рамках 1 таблицы
	* косистенотность отсутствует как и во всех OLAP
	* MVCC на уровне таблицы т.е. каждое изменение - новая вресия таблицы. Котораые надо чистить руками
	* поддержка схемы
	* много файлов с меты, которые могут лежать рядом с данными или во вне, в каталоге. Дучше во вне, это позволит переключать версии метаданных при изменении таблицы
	* durability делегируется сторонним сервисам
	* update через удаление и создание, опять на уровне таблицы т.е. сохдается новая версия
	* статистики меты собираются для compute движков
	* constraint не проверяет данные, это указания для движка расчета
## Как объединять данные из разных СУБД и делать это эффективно
* узкое место федеративных субд - коннекторы, нет pushdown предикатов и прочего

## Эволюция аутентификации в SSH
* аутентификация + безопасный транспорт
* аутентификация через пароли устарела, сильно
* ssh key - стандарт
	* компроментируется человеческим фактором (приватный ключ, хранится не безопасно)
* ssh pki - будущее
* яндекс отказался от LDAP, изобрели своё как всегда
* Yubikey - смаркарты для авторизации генерирующие публичный/приватный ключ каждый раз

## Распределённые системы без «зауми»
* CAP
* изначально для очень простой тех системы
* говрит P есть всегда ?
* Eventual consistency - общий стандарт
* CAP не подходит для больших сложных систем
* PACELC - расширенная версия, вводит больше понятий, блиэе к рельному миру
* полусинхронной транзакцией называют кврорум т.е. транзакция считается закоммиченнй когда произошла фиксация на большинстве нод
* классический CAP - для максимально атомарного звена системы, нельзя рассматривать взаимодествие систем, только по отдельности.
* Клепман раскритиковал классификацию по CAP

## Итак, вы решили надежно записывать данные на диск
* direct io - способ не обращаться в кеш ОС при записи, писать при записи
* у ОС есть кеш, у диска есть кэш, который теряется при потере питания
* flush и fsinc
* диск может наврать при записи, плохой, если не использовать fsync который не вернет управление пока не завишет данные на дисе гарантированно 
* проблема ошики фоновой записи на диск, нужен fsync
* fsync gate -  при падении fsync, а именно при ошибке IO база ложиться, чтобы кеш и диск не разьехались в данных
* posix
* разработчики файловых систем не предоставляют контракты для разработчиков приложений