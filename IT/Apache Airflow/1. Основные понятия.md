**Опр:** opensource платформа управления обработкой данных. 

Задачи/таски объединяются в DAG (Directed Acyclic Graph), где каждый узел графа - таск, каждое ребро - зависимость/поток данных. Исходя из определения в DAG не должно быть циклов т.е. замкнутых цепочек тасков (1->2->3->1).

Также есть способы отслеживать зависимости между DAG и запускать их в правильной последовательности т.е. не только по cron.

Компонеты:
**Scheduler** - запускает DAG-и
**Executor** - компонент, который определяет, как именно задачи будут выполняться. Есть много видов: локальные, для k8s и прочие. От выбора типа executor во многом зависит архитектура самого airflow приложения.

# YARN vs AirFlow
да, глупый вопрос, но он возник
*Yarn* - про управление/распределение/утилизацию ресурсов между  процессами Spark/MapReduce. Задача Yarn максимально эффективно распределить ресурсы среди всех задач вычислительного кластера.
*AirFlow* - запуск Tasks (это может быть всё что угодно, airflow - оркестратор) объединенных в DAG в правильной последовательности, максимально эффективно (в плане параллельности). 
Итого: airflow и yarn работают в разных экосистемах и абсолютно не взаимозаменяемы. Yarn не запускает таски и не отслеживает зависимости. Airflow не управляет ресурсами. 