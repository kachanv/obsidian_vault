* *Универсальность* - спарк развивает универсальное API для запуска распределенных заданий. Видимо прикло в том, что пож разные задачи, предоставляется единый интерфейс.
* *Движок расчетов* - это не место хранения (внезапно). Сосредотачиваются на том, чтобы смочь читать данные откуда угодно, обрабатывать их как угодно, и писать их куда угодно.
* *Библиотеки* -  универспльный API дает все возможности наплодить библиотек под любые языки (Java, Slcala, Python, R, SQL) и задачи (Stream, Batch, Graph, ML)

Spark создавался для мира дешевого storage, дорогого compute, распределенных многоузловых систем. В 2009 это была работа Matei Zaharia в униаерситете Беркли. В 2005 CPU перестали бурно расти по частоте год к году и наметился тренд на параллельное использование ядер или отдельных CPU, cost per storage продолжал снижаться, а размер данных - расти. Очевидно что нужен новый подход использующий эту парадигму: много CPU (дорогих) параллельно работающих над задачей (или задачами) с большими данными на дешевых (относительно) дисках + все это на распределенных сеоверах (в том числе чтобы быстро и просто масштабироваться)

* последние версии Spark **3.x**
* *DataBricks* - компания занимается развитием и коммерческим сопровождением Spark (и много чем ещё)
* большая часть написана на *Scala*
* https://spark-packages.org/ - доп пакеты от Spark комьюнити


