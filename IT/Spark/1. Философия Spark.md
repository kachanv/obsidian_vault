* *Универсальность* - спарк развивает универсальное API для запуска распределенных заданий. Видимо прикол в том, что под разные задачи, предоставляется единый интерфейс.
* *Движок расчетов* - это не место хранения (внезапно). Сосредотачиваются на том, чтобы смочь читать данные откуда угодно, обрабатывать их как угодно, и писать их куда угодно.
* *Библиотеки* -  универспльный API дает все возможности наплодить библиотек под любые языки (Java, Slcala, Python, R, SQL) и задачи (Stream, Batch, Graph, ML)

Spark создавался для мира дешевого storage, дорогого compute, распределенных многоузловых систем. В 2009 это была работа Matei Zaharia в университете Беркли. В 2005 CPU перестали бурно расти по частоте год к году и наметился тренд на параллельное использование ядер или отдельных CPU, cost per storage продолжал снижаться, а размер данных - расти. Очевидно что нужен новый подход использующий эту парадигму: много дорогих CPU, параллельно работающих над задачей (или задачами) с большими данными на относительно дешевых дисках + все это на распределенных серверах, в том числе чтобы быстро и просто масштабироваться.

* последние версии Spark **3.x**
* *DataBricks* - компания занимается развитием и коммерческим сопровождением Spark (и много чем ещё)
* большая часть написана на *Scala*
* https://spark-packages.org/ - доп пакеты от Spark комьюнити

**Развитие Spark**
1.x: 
* RDD based
* 1.3 появляется Structured API -  DataFrame, но не оптимизированное.
* Spark Streaming - микробатч 
* MLlib - ML на RDD
* ручные оптимизации RDD 
2.x: 
* разделение на batch и stream (новый stream API)
* появление DataSet как раздела Structured API
* разделение Structured API на DataSet и DataFrame под ЯП (Java | Python)
* появление Catalyst optimizer и Tungsten
* появился SparkSession
* возможность писать SQL запросы к данным
3.x: 
* runtime динамическая оптимизация плана выполнения
* Dynamic Partition Pruning - предотвращает сканирование ненужных разделов при чтении данных
* включение pandas API в Spark для Python
* Kubernetes интеграции


