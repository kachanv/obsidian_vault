# Компоненты 
далее речь пойдет о работе spark в cluster mode
## Spark application
**Опр:** Программа пользователя на Spark.
* конечная или бесконечная (stream/интерактивная оболочка) пользовательская программа spark объединенная общим контекстом.
* контекст вполне осязаем тк в рамках программы возможен шэринг данных без сохранения на внешние источники. т.е. каждое приложение spark - изолированно (по дефолту)
## Driver Process
**Опр:** процесс запущенный на узле кластера, управляющий выполнением отдельного Spark Application
он же просто *Driver*
* содержит всю мету в т.ч. пользовательский код в течении всей работы приложения
* запускается на одном из узлов кластера
* запрашивает ресурсы у *Claster Manager*
* преобразует пользовательский код в Job->Dag->Stage->Task и управляет отправлением Task-ов Executor-ам:
	* **Job** - атомарная параллельная цепочка действий в приложении. Каждый *action* в пользовательском коде Spark запускает свой *Job*. Например: *count()* | *save()* | *show()*. *action* всегда возвращает какой то результат. 
	* **Dag** - Job, разбитый на физический план выполнения в виде этапов ацикличного графа
	* **Stage** - пачка атомарных заданий, которые можно выполнять параллельно (для каждой партиции данных отдельный *Task*). Обычно совпадает с количеством *Shuffle*
	* **Task** - единица работы для *executor*-a над 1 партицией данных (для каждой партиции свой *Task* тк они могут быть выполнены параллельно)
* собирает результат, управляет кэшированием, глобальными счетчиками и переменными
## Cluster Manager
**Опр:** отдельный компонент, не являющийся частью "экосистемы" Spark по умолчанию. Эту роль может выполнять: Standalone cluster manager (def/built-in), Yarn, Mesos, Kubernetes etc.
* выделение, утилизация и мониторинг ресурсов кластера Spark приложению
* выделяет/разворачивает ноды/узлы/воркеры кластера для нужд spark application
* сначала к CM обращается пользовательское приложение (напр. *spark-submit*) за выделением ресурсов для *Driver* (1 отдельная нода)
* затем, когда *Driver* обработает пользовательский код и разложит его до Task, он обращается к CM для выделения ресурсов для *Executor*-ов (передавая конфиги для нод и их количество)
* после завершения *Spark Application* CM утилизирует все выделенные ресурсы
	* некоторые (большинство) *CM* поддерживают функцию динамического выделения и утилизации ресурсов т.е. меняют их прямо во время работы *application*
## Executor
**Опр:** процесс, непосредственно занимающийся выполнением Task-ов 
* на 1 узле/ноде/сервере/воркере может запускаться несколько Executors - это ключевой момент для понимания. Количество запущенных executor на 1 узле кластера зависит от свободных ресурсов этого узла. Узел в свою очередь предоставляет *Cluster Manager*
* выполнение *Task* от *Driver*
* сообщение *Driver*-у статуса *Task*
* пользовательский код на *Driver* может быть разным (Scala, Java, Python, SQL , R), но весь он взаимодействует с API Spark-а. т.е. в итоге на *Executor* все равно выполняется Spark code (JVM)
* у каждого *spark application* свои изолированные *executors*
* *executor* может выполнять несколько *Task* параллельно: 1 ядро CPU executor - 1 partition данных - 1 task. Поэтому вопрос партиционирования так важен в Spark.

![[Снимок экрана 2025-06-03 230919.png]]

![[Снимок экрана 2025-06-03 231256.png]]
# Трансформации
Каждая цепочка трансформаций представляет собой *DAG*
Разделяют на узкие и широкие (narrow & wide)
## Narrow
* выполняются на узле локально (без перемещения - *shuffle*)
* могут выполняться in memory т.е. без скидывания на диск
* 1 partition на входе и 1 на выходе
* пример: фильтр, чтение внешних данных (csv)
## Wide
* выполняются c перемещением - *shuffle*
* какие-то результаты все равно скидываются на диск перед или после перемещения
* N partition на входе или N на выходе
* пример: аггрегация
**Пример**
```python
# положим csv в DataFrame
flightData2015 = spark\ 
.read\ 
.option("inferSchema", "true")\ 
.option("header", "true")\ 
.csv("/data/flight-data/csv/2015-summary.csv")

# сделаем некий расчет
flightData2015\ 
.groupBy("DEST_COUNTRY_NAME")\ # группировка будет wide трансформацией
.sum("count")\ 
.withColumnRenamed("sum(count)", "destination_total")\ 
.sort(desc("destination_total"))\ 
.limit(5)\ 
.explain()
```
`option("inferSchema", "true")` - при чтении внешних данных можно дать Spark-у на откуп самому определить тип данных (*schema-on-read*), и это будет narrow трансформация.

*explain()* - вызывает физический план выполнения цепочки операций. Читается снизу вверх. Этот план, в целом, независим от языка, на котором написаны команды (в общем, с оговорками). Получим такой план: создано 6 промежуточных DataFrame, по 1 для каждой трансформации
```
== Physical Plan == TakeOrderedAndProject(limit=5, orderBy=[destination_total#16194L DESC], outpu...   
	+- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[sum(count#7325L)]) 
		+- Exchange hashpartitioning(DEST_COUNTRY_NAME#7323, 5) 
			+- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[partial_sum...
				+- InMemoryTableScan [DEST_COUNTRY_NAME#7323, count#7325L] 
					+- InMemoryRelation DEST_COUNTRY_NAME#7323, ORIGIN_COUNTRY_NA... 
						+- *Scan csv [DEST_COUNTRY_NAME#7578,ORIGIN_COUNTRY_NAME...
```
Видим 2 аггрегации: 1 внутри каждой партиции, 1 общая (с shuffle)
### Shuffle
**Опр:** перемещение данных между узлами кластера для выполнения определенных операций: *join* | *group by* | *repartition* | *collect* | *coalesce* и все то, что нельзя посчитать локально
* дорогая операция
* первоначально для обработки на каждый executor попадает свой набор partition, затем для продолжения обработки приходится перемещать данные м/у узлами кластера (см. [[#S3 operation]])
* важно что данные перемещаются между узлами кластера (на которых находятся executors) напрямую. Без посредника - буффера.
* перед каждым shuffle spark записывает данные на диск узла кластера
* перед shuffle данные можно сжимать (настраивается в *spark-submit*)
* в плане *Shuffle* обозначается как *Exchange*
# Структуры данных
* основные структуры данных иммутабельны т.е. при трансформации получаем новый объект. Этот новый объект будет наследовать доступные параметры (типы данных) от родителя. То есть меняя какой то DataFrame, под капотом создается новый. Но не в момент объявления, а только когда к нему обратятся (см.Lazy Evalution)
## RDD
**Опр:** Resilient Distributed Dataset - устойчивая/надежная распределенная коллекция типа таблицы
* изначальная структура данных в Spark
* на ней основаны более высокие абстракции (*DataSet*, *DataFrame*)
## DataFrame
**Опр**: table-like структура данных для работы (в основном) в Python и R
* Pandas dataframe это другое, похожее но все же другое, просто называются одинаково
* Dataframe это Dataset типа row
## DataSet
**Опр**: row-like структура данных для работы в Scala и Java
# Концепции
## Lazy Evalution
* Spark запускает выполнение задачи только тогда, когда поступает команда на выполнение графа задач (*Action*)
* оптимизирует граф (PushDown предикатов и все такое)
* т.е. пока какой либо объект не нужен , он не создается. Если объявить в коде dataframe, dataset но не использовать его, spark не будет его создавать (сохранение объекта на диск это считается использованием)
## Schema
**Опр**: определяет структуру данных - колонки и типы, для DataFrame и DataSet.
* DataFrame состоит из objects с type - *Row*, которые внутри представляют собой массив байт. К отдельным частям *Row* можно обращаться при помощи *columns* или *col*.
* *Row* не имеют схемы, схема закреплена за *DataFrame*
* доступ к элементу *Row* осуществляется по индексу (порядковому номеру в массиве)
* можно задать тип элементам *Row*
## S3 operation
как spark работает с данными, ведь в отличии от GP где данные физически лежат "под рукой", здесь данные размазаны по распределенному серверу.
Верхнеуровневый алгоритм
* *driver* смотрит мету данных S3 - понимает формат, количество файлов, определяется с количеством партиций. Здесь *Партиция* - просто кусок данных, в зависимости от формата это могут быть просто файлы или готовые партиции Iceberg.
* *driver* назначает *каждому* executor N партиций. Далее партиции по сети скачиваются на узел spark кластера, который *kubernetes* развернул под наш расчет. Помним что на 1 узле может быть несколько *executor*. Т.о. идет параллельное скачивание по сети.
* в плане запроса можно увидеть `Scan [Format][Path]`
Здесь же стоит отметить что в если мы оперируем Spark SQL, то spark различает *managed* и *unmanaged* таблицы
* *unmanaged* - таблицы, в которых spark управляет только метаданными. Любые таблицы с внешних источников: S3/HDFS  etc. и это в концепции spark, как инструмента compute - все первоначальные источники данных. Обычно метой таких таблиц управляет отдельный сервис - каталог, напр. Nessie. Чтобы создать такую таблицу используется `create external table ... location ...`
* *managed* - таблицы, в которых spark управляет метой и физикой таблицы. Если просто создать таблицу `create table` - это будет именно такая таблица. Данные физически лягут на кластере Spark.
# Всякая всячина
## spark-submit
**Опр:** CLI
`./bin/spark-submit`
* запуск spark application с помощью командной строки, так запускаются даги в боевой среде
* как альтернатива и интерактивный режим, на разработке может использоваться *spark shell* | *pyspark* | *spark-sql*
* запускает py/jar файлы
* имеет довольно много аргументов
## Structured Streaming API
**Опр:** high-level API для создания stream (не batch) джобов. Со Spark 2.2. 
* поддерживает многие Batch методы для работы в Stream режиме
* триггер - что событие, которое отследивает Spark для обновления
Пример:
```python
# создаем DataFrame из csv
streamingDataFrame = spark.readStream\
.schema(staticSchema)\
.option("maxFilesPerTrigger", 1)\ # ключевой параметр - триггер, что является событием для обновления
.format("csv")\
.option("header", "true")\
.load("/data/retail-data/by-day/*.csv")

# создвем in-memory таблицу, обновляемую maxFilesPerTrigger
purchaseByCustomerPerHour.writeStream\
.format("memory")\
.queryName("customer_purchases")\
.outputMode("complete")\
.start()

# обращаемся к in-memory таблице с помощью SQL
spark.sql("""
SELECT *
FROM customer_purchases
ORDER BY `sum(total_cost)` DESC
""")\
.show(5)
```
## Machine Learning & Advanced Analytics
* *Spark MLlib* из коробки - либа для ML
* есть классификации, регресси и deep learning и ещё куча всего
* много оптимизаций: кэширование и пр.
поскольку сейчас я бесконечно далек от ML, конспектировать эту часть не стал.
## Lower-Level APIs
**Опр:** несколько API для оперирования низкоуровневыми примитивами на базе *RDD* с помощью доступных ЯП.
* позволяет управлять физическими параметрами выполнения, в отличие от *high-level API*
* доступен из *Scala* и *Python*, но в отличие от *high-level API*, имеются нюансы реализации
* утверждается что в современных версиях Spark нет необходимости лезть в *low-level API*
## SparkR
**Опр:** эта и многие другие либы доступны для любитей математики и пописать на R что-нибудь особенно заковыристое. 
* имеет схожий с классическим R синтаксис
* выичсления выполняются распределенно и код в итоге проеобразуется в RDD инструкции
* поддерживается также *sparklyr*
## PySpark
некоторые особенности
* можно использовать *Pandas*, предварительно собрав все данные на *driver*
