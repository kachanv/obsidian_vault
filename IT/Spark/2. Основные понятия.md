# Компоненты 
## Cluster Manager
Standalone cluster manager, Yarn, Mesos, etc.
* выделение, утилизация и мониторинг ресурсов кластера Spark приложению
## Driver Process
* сердце Spark приложения, содержит всю мету в течении всей работы приложения
* запускается на одном из узлов кластера
* отвечает за испольняюмую программу пользователя
* планирование работы Executors
* *SparkSession* - драйвер с которым можно взаимодействовать в интерактивном режиме
## Executors
на 1 узле может запускаться несколько Executors
* выполнение задач от Driver
* сообщение Driver-у статуса задачи
* выполняемый код может быть разным (Scala, Java, Python, SQL 2003 standard, R: SparkR и sparklyr), но весь он взаимодействует с API Spark-а. т.е. в итоге все равно выполняется Spark code (JVM)
* API разделяют на 2 уровня: *low level* - инструкции и *high level* - абстракции (*Structured API*)

![[Снимок экрана 2025-01-27 211405.png|500x360]]

# Трансформации
Каждая цепочка трансформаций представляет собой *DAG*
Разделяют на узкие и широкие (narrow & wide)
## Narrow
* выполняются на узле локально (без перемещения - *shuffle*)
* могут выполняться in memory т.е. без скидывания на диск
* 1 partition на входе и 1 на выходе
* пример: фильтр, чтение внешних данных (csv)
## Wide
* выполняются на узле локально (c перемещением - *shuffle*)
* какие-то результаты все равно скидываются на диск перед или после перемещения
* N partition на входе или N на выходе
* пример: аггрегация
**Пример**
```python
# положим csv в DataFrame
flightData2015 = spark\ 
.read\ 
.option("inferSchema", "true")\ 
.option("header", "true")\ 
.csv("/data/flight-data/csv/2015-summary.csv")

# сделаем некий расчет
flightData2015\ 
.groupBy("DEST_COUNTRY_NAME")\ 
.sum("count")\ 
.withColumnRenamed("sum(count)", "destination_total")\ 
.sort(desc("destination_total"))\ 
.limit(5)\ 
.explain()
```
`option("inferSchema", "true")` - при чтении внешних данных можно дать Spark-у на откуп самому определить тип данных (*schema-on-read*), и это будет narrow трансформация.

*explain()* - вызывает физический план выполнения цепочки операций. Читается снизу вверх. Этот план, в целом, независим от языка, на котором написаны команды (в общем, с оговорками). Получим такой план: создано 6 промежуточных DataFrame, по 1 для каждой трансформации
```
== Physical Plan == TakeOrderedAndProject(limit=5, orderBy=[destination_total#16194L DESC], outpu...   
	+- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[sum(count#7325L)]) 
		+- Exchange hashpartitioning(DEST_COUNTRY_NAME#7323, 5) 
			+- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[partial_sum...
				+- InMemoryTableScan [DEST_COUNTRY_NAME#7323, count#7325L] 
					+- InMemoryRelation DEST_COUNTRY_NAME#7323, ORIGIN_COUNTRY_NA... 
						+- *Scan csv [DEST_COUNTRY_NAME#7578,ORIGIN_COUNTRY_NAME...
```
Видим 2 аггрегации: 1 внутри каждой партиции, 1 общая.
# Структуры данных
* основные структуры данных иммутабельны т.е. при трансформации получаем новый объект. Этот новый объект будет наследовать доступные параметры (типы данных) от родителя.
## RDD
**Опр:** Resilient Distributed Dataset - устойчивая/надежная распределенная коллекция типа таблицы
* изначальная структура данных в Spark
* на ней основаны более высокие абстракции (*DataSet*, *DataFrame*)
## DataFrame
**Опр**: table-like структура данных для работы (в основном) в Python и R
## DataSet
**Опр**: row-like структура данных для работы в Scala и Java
# Концепции
## Lazy Evalution
* Spark запускает выполнение задачи только тогда, когда постуапет команда на выполнение графа задч (*Action*)
* оптимизирует граф (PushDown предикатов и все такое)
## Schema
**Опр**: определяет структуру данных - колонки и типы, для DataFrame и DataSet.
* DataFrame состоит из objects с type - *Row*, которые внутри представляют собой массив байт. К отдельным частям *Row* можно обращаться при помощи *columns* или *col*.
* *Row* не имеют схемы, схема закреплена за *DataFrame*
* доступ к элементу *Row* осуществляется по индексу (порядковому номеру в массиве)
* можно задать тип элементам *Row*

# Всякая всячина
## spark-submit
**Опр:** CLI
`./bin/spark-submit`
* запуск джобов с помощью командной строки
* на любом доступном ЯП
## Structured Streaming API
**Опр:** high-level API для создания stream (не batch) джобов. Со Spark 2.2. 
* поддерживает многие Batch методы для работы в Stream режиме
* триггер - что событие, которое отследивает Spark для обновления
* *как работают executors и driver в stream реиме? т.е. не утилизируются ? или инициализируются на каждый триггер*
Пример:
```python
# создаем DataFrame из csv
streamingDataFrame = spark.readStream\
.schema(staticSchema)\
.option("maxFilesPerTrigger", 1)\ # ключевой параметр - триггер, что является событием для обновления
.format("csv")\
.option("header", "true")\
.load("/data/retail-data/by-day/*.csv")

# создвем in-memory таблицу, обновляемую maxFilesPerTrigger
purchaseByCustomerPerHour.writeStream\
.format("memory")\
.queryName("customer_purchases")\
.outputMode("complete")\
.start()

# обращаемся к in-memory таблице с помощью SQL
spark.sql("""
SELECT *
FROM customer_purchases
ORDER BY `sum(total_cost)` DESC
""")\
.show(5)
```
## Machine Learning & Advanced Analytics
* *Spark MLlib* из коробки - либа для ML
* есть классификации, регресси и deep learning и ещё куча всего
* много оптимизаций: кэширование и пр.
поскольку сейчас я бесконечно далек от ML, конспектировать эту часть не стал.
## Lower-Level APIs
**Опр:** несколько API для оперирования низкоуровневыми примитивами на базе *RDD* с помощью доступных ЯП.
* позволяет управлять физическими параметрами выполнения, в отличие от *high-level API*
* доступен из *Scala* и *Python*, но в отличие от *high-level API*, имеются нюансы реализации
* утверждается что в современных версиях Spark нет необходимости лезть в *low-level API*
## SparkR
**Опр:** эта и многие другие либы доступны для любитей математики и пописать на R что-нибудь особенно заковыристое. 
