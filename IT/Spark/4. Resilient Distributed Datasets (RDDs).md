**Опр:** структура данных и вместе с тем API. Неизменяемая, распределенная, отказоустойчивая коллекция объектов, которая может быть обработана параллельно.
* самая низкоуровневая структура данных в spark
* каждая строка *RDD* - java/python объект => может содержать что угодно. В то время как *Structured API* оперирует типами Spark
* доступ к rdd может быть осуществлен с помощью Python/Java/Scala
* Dataframe/DataSet можно конвертировать в RDD (точнее применять к ним методы RDD API). И также наоборот. Т.о. можно переключаться и использовать методы Low/High level API
* манипуляции над RDD разделяются на *Transormation* (filter, distinct, map) и *Action* (reduce, count)
### Low level API
**Опр:** API позволяющее взаимодействовать с низкоуровневыми объектами Spark напрямую.
* все операции со *Structured API* в итоге конвердируются в RDD операции
* в свою очередь включает управление данными и управление переменными
* утверждается что в текущих версиях Spark практически нет необходимости обращаться к нему
**Use cases:**
* нужен контроль над физическим размещением данных на кластере (*custom partitioning*)
* нужен функционал не покрывающий *Structured API*
* нужно напрямую влиять на выполнение пайплайна обработки
* нужно ввести свой тип данных и определить операции над ним
### SparkSession и SparkContext
*Driver*, *Executor*, *Task* - эти сущности выпоняют какие-то роли для работы Spark приложения. Но в пользовательском коде мы не обращаемся к ним напрямую, далее рассмотрим центральные сущности/классы/методы API Spark с которыми мы взаимодействуем в коде.
Центральные сущности Spark, для создания spark application и обращения к Structured API и RDD
**SparkSession**
**Опр:** унифицированная централизованная точка взаимодействия со Structured API. Раздел Spatk API.
* первый щаг при создании Spark Aplication это создание SparkSession `SparkSession.builder()`
* введен в Spark 2.0. а до того приходилось пользоваться по отдельности SQLContext, HiveContext, SparkContext
* создает и настраивает SparkContext
* разворачивает Spark Application
* остановка полностью прекратит работу приложения
```python
# Initialize SparkSession
spark = SparkSession.builder.appName("Example").getOrCreate()

# Read data → Transform → Query
df = spark.read.json("data.json")
df_filtered = df.filter(df.age > 30)
df_filtered.createOrReplaceTempView("people")
results = spark.sql("SELECT * FROM people WHERE age > 30")

# Write output
df_filtered.write.parquet("output.parquet")

# Stop session
spark.stop()
```
**SparkContext**
**Опр:**  унифицированная централизованная точка взаимодействия со RDD API.
* управление ресурсами, переменными, выполнением пользовательского spark application
* введен в Spark 1.0
* все обращения к RDD идут через него
```python
sc = spark.sparkContext
rdd = sc.parallelize([1, 2, 3, 4, 5], 2)
```
### Некоторые фичи
* `checkpoint` - принудительное сохранение RDD на диск
* `pipe` - делегирование операций над rdd стороннему сервису. Например этап обработка данных вне spark ML моделью
* кастомная сериализация - любой объект Spark перед расчетом проходит через сериализацию в Java объект что имеет довольно большой overhead (решается включением Tungsten). Можно задать кастомную сериализацию.
### Custom Partitioning
самый популярный способ обратиться к RDD
* Struсtured API позволяет менять партиционирования по списку (Hash) и диапазонам (Range)
* используя rdd можно партиционировать по кастомному правилу и, например, решить проблему перекоса по данным при расчете.