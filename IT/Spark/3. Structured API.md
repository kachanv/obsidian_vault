* high-level API
* включает в себя 3 api для работы с распределенными коллекциями: DataFrame, Dataset and SQL tables
* любая библиотека использует какой-то 1 тип API, жонглировать ими на ходу не получится
# Выполнение кода
Ход преобразования кода джоба в команды driver-у.
Как выполняется пользовательский код на Structured API: 
	**Structured API код джоба -> Logical Plan - > Physical Plan**
 *Catalyst* - cost based ортимизатор, строит планы, оценивает, выбирает самый дешевый.
 * в т.ч. производит push-down предикатов
 * можно настроить кастомный rule-based оптимизатор
![[Снимок экрана 2025-02-18 220555.png]]
## Логический план
**Опр:** оптимизированный верхнеуровневый код джоба сопоставленный с данными
**User Code -> Unresolved Logical Plan -> Resolved Logical Plan -> Optimized Logical Plan**
где 
* *Unresolved Logical Plan* - логический план не сматчен с данными. 
* *Resolved Logical Plan* - сервис *analyzer* сходил в *Catalog* и сматчил план с данными и их типами. Если чего то не оказалось (поля или объекта) - ошибка 
* *Optimized Logical Plan* - *Catalist* взял план с предыдущего шага, применил оптимизации опираясь на данные и план операций, это всё еще Structured API код
![[Снимок экрана 2025-02-18 220638.png]]
## Физический план 
**Опр:** RDD код - преобразованный Structured API из Optimized Logical Planx. Его показывает нам explain он же Spark план.
* cost-based выбор оптимального физического плана
* сюда входят: тип соединения (физический тип join), движение данных кластеру и пр.
![[Снимок экрана 2025-02-18 223401.png]]
## Tungsten 
**Опр:** внутреннее расширение движка обработки устраняющее некоторые bottle neck-и характерные для spark приложений, а именно использование RAM и CPU.
* по умалчанию включен в Spark 2.0+
* включает очень низкоуровневые оптимизации
* оптимизирует сериализацию/десериализацию, кэширование JVM объектов и всякие характерные для java штуки, как garbage collection.
# Коллекции
## DataFrame
* табличная структура, вертикально шардируемая по узлам кластера/нодам (важно, не по Executor-ам !)
* используется в основном в Python, R (из Java/Scala тоже можно если нужно)
* иммутабельная структурированная распределенная table-like коллекция (столбцы и строки), есть типизация
* соответствие типов данных проверяется во время выполнения, а не компиляции => при несоответствии типов код упадет во время выполнения, а не компиляции
* *Partition* - как именно будут шардированы данные (речь про шардирование во время обработки данных spark-ом, а не шардированя в системе хранения)
* нельзя работать к конкретной партицией, только со всем DataFrame. У *RDD* - можно используя Low level Spak API
- есть sql-like интерфейс (join, filter etc.)
- хорошая оптимизация для работы с большими объемами
- не смотря на то что в R и Python нет строгой типизации при взаимодействии с данными мы оперируем структурами Spark-а (DataFrame) со строгой типизацией
### Некоторые функции
синтакис из *pyspark*
- несколько последовательных функций `filter` или `where` будут выполненны одновременно, порядок не имеет значения
	  `df.filter(col("count") < 2).filter(col("flg") = 2)`
	  для использования логического ИЛИ в фильтр  - `|`
	  `where(Filter1 | Filter2)`
* `sample` - возвращает рандомный набор строк из df
* `randomSplit` - разбивает df на n кусков рандомным образом, не обязательно равных
* `union` - соединяет 2 df, обязательно с одинаковой схемой и колличеством столбцов, результатом будет отдельный df (тк df - immutable). В некоторых версиях Spark важна не схема, а порядок столбцов
* `sort`| `orderBy` - сортировка. Присутствуют asc/desc и порядок null. По умолчанию строки сортируются в рамках всего df. Для сортировки внутри партиций - `sortWithinPartitions`
	  `df.sort(expr("count desc"))`
* `repartition` - изменение структуры партиций по количеству или по выражению. Приведет к shuffle всех строк df.
	```python
	df.rdd.getNumPartitions() #получить текущее количество партиций
	df.repartition(5) #разбить на 5 партиций
	df.repartition(col("DEST_COUNTRY_NAME")) #разбить по выражению столбца, каждое значение пойдет в свою партицию
	df.repartition(5, col("DEST_COUNTRY_NAME")) #разбить по выражению столбца, но ограничить кол-во партиций
	```
* `coalesce` - объединение партиций на ноде кластера. Не приводит к shuffle всех данных
	`df.coalesce(2)` - объединит всё в 2 партиции
* `collect` - переносит df на Driver => нужно быть осторожным в объемах
	`toLocalIterator` - итератор позволяющий переносить и обрабатывать df на driver по partition
* `equalTo` - сравнение равенства 
	```python
	df.where(col("InvoiceNo").equalTo(536365))
	df.where("InvoiceNo = 536365") # аналогично
	```
* `eqNullSafe` - аналог IS DISTINCT FROM 
* `corr` - есть функции мат статистики, например коэффициент корреляции Пирсона м\у двумя колонками 
* `describe` - вохвращает базовую статистику для столбцов таблицы
	`df.describe().show()`
* `regexp_extract` | `regexp_replace` - вычисление регулярок, но синтаксис re наследуется из Java т.е. отличается от классического
* `count` - классическая аггрегация. `countDistinct` - только уникальные. `approx_count_distinct` - приблизительный расчет
    `df.select(count("StockCode")).show()` - вернет количество строк где StockCode is not null
* есть много статистических функций, доступных как в py/java так и в SQL: среднеквадратичное отклонение, коррекляции, ковариации, коэффициенты ассиметрии и эксцесса и т.п.
* `groupBy` - классическая группировка. Есть специальный синтаксис для window группировки
```python
from pyspark.sql.functions import count
df.groupBy("InvoiceNo").agg(
count("Quantity").alias("quan"), #одинаковый результат
expr("count(Quantity)")).show() #одинаковый результат
```
* `pivot`- разворачивает строку в колонку т.е. каждое унакльное значение строки будет колонкой. Используется в комбинации с группировками.
* `explain` - возвращает физический план запроса. Можно добавить логический план и стоимость каждой операции. Читается по правилам обычного SQL плана
* `df.cache()` | `CACHE TABLE` - лениво помещает таблицу в память кластера т.е. не сразу а при первом чтении/обращении. Есть много политик/настроек по spill на диск и т.п.
#### User-Defined Functions
* то, за что особенно любят spark
* udf пишется как обычная f-я на соответствующем ЯП, без особенностей. Затем после регистрации - `spark.udf.register()` udf попадает на *driver* и *executor*.
* обрабатывают df row by row (есть способы обрабатывать пачками - *Vectorized UDF*)
* могут быть написаны на scala | java | python но есть отличия
	* **JVM**: f-я нативно выполняется на каждом executor без потерь производительности тк там есть jvm
	* **Python**: spark создает py процесс на каждом executor, сериализует все типы данных jvm в типы данных py (и обратно), выполняет функцию, проводит обратную сериализацию. Плюс jvm не контролирует потребление ресурсов py процессом и можно утилизировать все ресурсы воркета таким образом.
* создание агрегирующих UDF функций доступно только из scala/java
* считается что любая UDF - bottleneck и их следует избегать (по возможности) - использовать функции *Structured API*
## Dataset
- иммутабельная структурированная распределенная table-like коллекция (столбцы и строки), строгая типизация
- Java/Scala
- сочетает фишки из RDD и DataFrame
- поддержка схемы (типы/столбцы)
- соответствие типов данных проверяется во время компиляции => код более безопасен
- хорошо подходит для задач агрегации, оконные функции
- поскольку Python/R динамически типизированные ЯП оперировать в них DataSet-ами не получиться
### Dataset API
**Опр:** API типов для ЯП со трогой типизацией
* позволяет засунуть строку DataFrame в Scala/Java класс
* обеспечивает *type safety*
* недоступен для Python и R, точнее лишен смысла
* *Пример*: объявляем свой Dataset[Person], каждая строка DataFrame - экземпляр класса Person, гарантированно содежращий класс Person. Реализуем бизнес логику джоба на Scala/Java. При этом все манипуляции будут проводится не с оригинальным DataFrame, а с Dataset. Результат расчета можно снова крутить как DataFrame.
### Use cases
какие плюшки дает использование Dataset
* расширенный функционал Dataset API
* строгая типизация (type safety)
* UDF производительнее на Scala/Java (относительно Python)
* нужно писать код в стиле ООП, или функционального программирования (lambda)
## SQL Tables
Все что позволяет делать это API - выполнять SQL код над DataFrame и Dataset
```scala
df.createOrReplaceTempView("people")
val sqlResult = spark.sql("SELECT name, age FROM people WHERE age > 30")
```
# Типы данных
некоторые особенности и необычные типы
* **важно:** по умолчанию если spark получает в "таблице" тип данных не соответсвующий схеме (например 'abc' вместо integer) или если не может автоматически привести типы даннх при каком-то расчете (например сравнивает integer и 'abc') он присвоит соотвествующему значению поля null, а не вернет ошибку.
## Дата и время
* существуют 2 типа данных работающих c временем и датой непосредственно: `timestamp` и `date`
* `timestamp` не поддрживает точность более чем second т.е. нет милисекунд и т.п. Есть workaround - мс хранятся отдельно как long
* если spark нет может конвертировать строку в дату/время  он не выкинет ошибку, он вернет null в значении (если в схеме поле nullabe)
## Работа с Null
* объявление колонки df not null на уровне схемы не включает сооотвествкющего constraint-а т.е. можно вставить null в not null колонку df. Т.о. Spark не обеспечивает и не проверяет соблюдение null спецификации/контракта схемы df. Даже больше, *при наличии null в not null полях допускается некорректный расчет или падение с непонятным исключением.*
## Struct
* df внутри df. В ячейке таблицы может лежать отдельный df. Тип данных в таком случае будет `complex`
## Array
* list - классический массив значений. Богатый набор операций над массивами: `split`, `size`, `array_contains`, `explode`
## Map
* dict | key-value. Можно создать из 2 колонок таблицы или из выражения группировки
# Операции
## Group by
помимо стандартной группировки *Group by* есть такие фичи (знакомые из MsSQL):
### Grouping sets
доступны только в SQL, позволяют проводить группировку сразу в нескольких измерениях
```sql
select A, B, count(*) from table group by grouping sets((A), (A, B), (B))
```
результатом будет 3 колонки, в столбцах A и B будут все комбинации из table + отдельные группировки по A и по B с null в соотвествующей колонке.
Запрос будет аналогичен следующему
```sql
select A, null as B, count(*) from table group by A
union all
select A, B, count(*) from table group by A, B
union all
select null as A, B, count(*) from table group by B
```
Для корректного результа ориганальная table не должна содержать null в измерениях.
### Rollup
также позволяет проводить группировку сразу в нескольких измерениях, но здесь уже важен порядок полей в секции rollup
```sql
select A, B, count(*) from table group by rollup(A, B)
```
результатом будет 3 колонки, в столбцах A и B будут все комбинации из table + отдельная группировки по A с null в B + общий TOTAL по всей таблице. Поэтому важен порядок полей в секции.
Запрос будет аналогичен следующему
```sql
select A, null as B, count(*) from table group by A
union all
select A, B, count(*) from table group by A, B
union all
select null as A, null as B, count(*) from table 
```
Для корректного результа ориганальная table не должна содержать null в измерениях
### Cube
проводт группировку сразу во всех измерениях. т.е. по всем комбинациям полей с null в соотвествущем поле.
```sql
select A, B, count(*) from table group by cube(A, B)
```
результатом будет 3 колонки, в столбцах A и B будут все комбинации из table + отдельная группировки по A с null в B + отдельная группировки по B с null в A + общий TOTAL по всей таблице.
Запрос будет аналогичен следующему
```sql
select A, null as B, count(*) from table group by A
union all
select A, B, count(*) from table group by A, B
union all
select null as A, B, count(*) from table group by B
union all
select null as A, null as B, count(*) from table 
```
## Join
BTW: в качестве условия join-а может быть вхождения ключа в массив
### Типы Join
кроме привычных
**LEFT SEMI**
```sql
SELECT * FROM T1 LEFT SEMI JOIN T2 ON T1.id = T2.id
```
Оставит только столбцы и ключи из T1 у которых есть совпадение в T2. Аналогично фильтру на IN
**LEFT ANTI**
```sql
SELECT * FROM T1 LEFT ANTI JOIN T2 ON T1.id = T2.id
```
Аналогично, только с несовпавшими ключами
### Стратегии Join
* то что можно увидеть в плане запроса.
* далее Т-таблица или партиция таблицы на определенном executor
* определяется Catalist
* можно задать с помощью хинта
**Broadcast Hash Join**
* одна T копируется на все ноды (в первую очередь на *driver*) и помещается в память в виде hash map
* затем executor сравнивает каждую строку своей партиции с hash Т т.е. классический hash join
* не требует shuffle - это плюс
* одна из таблиц должна быть кратно меньше другой и влезать в RAM, по дефолту 10Мб
```sql
SELECT /*+ BROADCAST(smallTable) */ *
FROM largeTable
JOIN smallTable ON largeTable.key = smallTable.key
```
**Shuffle Hash Join**
* сначала происходит shuffle данных c партиционированием по ключу join-а так чтобы данные с одним ключем оказались на 1 ноде кластера
* затем классический hash join с построением hash map на меньшую Т
* требует shuffle двух таблиц - это минус
* требует построения hash map для большой T
* отключен по дефолту
* hint: `/*+ SHUFFLE_HASH(table) */`

**Sort Merge Join**
* первый шаг аналогичен предыдущему
* затем сортировка двух Т по общему полю, а после Merge join
* требует shuffle двух таблиц - это по прежнему минус
* должно быть общее поле сортировки
* потребляет меньше памяти тк hash map не строится (по больше CPU из-за сортировки)
* join по default
* есть встроенные оптимизации обработки перекошенных соединений `spark.sql.adaptive.skewJoin.enabled=true`
* hint: `/*+ MERGEJOIN(table) */`

**Broadcast Nested Loop Join**
* аналогичен *Broadcast Hash Join* только на каждой executor происходит *Nested Loop* соединение - т.е. попарное сравнение строки большой Т со всеми строками меньшей Т.
* выбирается когда никакой другой способ не возможен  из-за условия соединения join-а (больше/меньше и т.п.)
* самый медленный

**Cartesian Join**
* производит декартово произведение - cross join
* очень дорогой и медленный: O(n * m)
* производится shuffle, чтобы каждая партиция T1 могла поджоинится с каждой партицией T2
* nested loop 
