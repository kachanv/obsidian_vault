* high-level API
* включает в себя 3 api для работы с распределенными коллекциями
* любая библиотека использует какой-то 1 тип API, жонглировать ими на ходу не получится
# Выполнение кода
Ход преобразования кода джоба в команды driver-у.
Как выполняется пользовательский код на Structured API: 
	**Structured API код джоба -> Logical Plan - > Physical Plan**
 *Catalyst* - cost based ортимизатор, строит планы, оценивает, выбирает самый дешевый.
 * в т.ч. производит push-down предикатов
 * можно настроить кастомный rule-based оптимизатор
![[Снимок экрана 2025-02-18 220555.png]]
## Логический план
**Опр:** оптимизированный верхнеуровневый код джоба сопоставленный с данными
**User Code -> Unresolved Logical Plan -> Resolved Logical Plan -> Optimized Logical Plan**
где 
* *Unresolved Logical Plan* - логический план не сматчен с данными. 
* *Resolved Logical Plan* - сервис *analyzer* сходил в *Catalog* и сматчил план с данными и их типами. Если чего то не оказалось (поля или объекта) - ошибка 
* *Optimized Logical Plan* - *Catalist* взял план с предыдущего шага, применил оптимизации опираясь на данные и план операций, это всё еще Structured API код
![[Снимок экрана 2025-02-18 220638.png]]
## Физический план 
**Опр:** RDD код - преобразованный Structured API из Optimized Logical Plan
он же Spark план.
* cost-based выбор оптимального физического плана
* сюда входят: тип соединения (физический тип join), движение данных кластеру и пр.
![[Снимок экрана 2025-02-18 223401.png]]
# Коллекции
## DataFrame
* python, R (из Java/Scala тоже можно если нужно)
* таблица, вертикально шардируемая по узлам кластера (важно, не по Executor-ам !)
*  иммутабельная структурированная распределенная table-like коллекция (столбцы и строки), есть типизация, разные типы данных
* соответствие типов данных проверяется во время выполнения, а не компиляции
* *Partition* - как именно будут шардированы данные (1 Executor = 1 parallel lvl, 1 Partition = 1 parallel lvl)
* нельзя работать к конкретной партицией, только со всем DataFrame. У *RDD* - можно используя Low level Spak API
- есть sql-like интерфейс (join, filter etc.)
- хорошая оптимизация для работы с большими объемами
- не смотря на то что в R и Python нет строгой типизации при взаимодействии с данными мы оперируем структурами Spark-а (DataFrame) с типизацией
-  С точки зрения спарк, обращение к колонке DataFrame - выражение (*expression*) т.е. `(((col("someCol") + 5) * 200) - 6) < col("otherCol")` полностью аналогитчно `expr("(((someCol + 5) * 200) - 6) < otherCol")`
- 
## Dataset
- Java/Scala
- иммутабельная структурированная распределенная table-like коллекция (столбцы и строки), строгая типизация
- сочетает фишки из RDD и DataFrame
- поддержка схемы (типы/столбцы)
- соответствие типов данных проверяется во время компиляции
- хорошо подходит для задач аггрегации, оконные функции
## SQL Tables
* [empty]
## Dataset API
**Опр:** API типов для ЯП со трогой типизацией
* позволяет засунуть строку DataFrame в Scala/Java класс
* обеспечивает *type safety*
* недоступен для Python и R, точнее лишен смысла
* *Пример*: объявляем свой Dataset[Person], каждая строка DataFrame - экземпляр класса Person, гарантированно содежращий класс Person. Реализуем бизнес логику джоба на Scala/Java. При этом все манипуляции будут проводится не с оригинальным DataFrame, а с Dataset. Результат расчета можно снова крутить как DataFrame.

# Операции
я

78